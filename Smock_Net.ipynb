{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import pandas as pd \n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from itertools import product\n",
    "import random\n",
    "\n",
    "\n",
    "# data_root = \"C:\\\\Users\\\\Tigran PC\\\\Desktop\\\\MicrobiomicsData\\\\data\\\\\" # notebook\n",
    "# data_root = \"/Users/tigran/Desktop/sbv/data/\" # imac\n",
    "data_root = \"D:\\\\sbv Microbiomics\\\\data\\\\\" # istc pc\n",
    "\n",
    "sub_data_root = data_root + \"sample01split/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reformat_lables(labels):\n",
    "    num_labels = len(np.unique(labels))\n",
    "    # Map 0 to [1.0, 0.0, 0.0 ...], 1 to [0.0, 1.0, 0.0 ...]\n",
    "    labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "    return labels\n",
    "\n",
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1)) / predictions.shape[0])\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First reload the data we generated in Feature_Engineering.ipynb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (13244, 257)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = data_root + \"DNA_data.pickle\"\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    save = pickle.load(f)\n",
    "    all_dataset = save.astype(np.float32)\n",
    "    del save  # hint to help gc free up memory\n",
    "    print('Training set', all_dataset.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (10595, 256) (10595,)\n",
      "Validation set (1324, 256) (1324,)\n",
      "Test set (1325, 256) (1325,)\n"
     ]
    }
   ],
   "source": [
    "X = all_dataset.values[:, :-1]\n",
    "y = all_dataset.values[:, -1]\n",
    "\n",
    "train_dataset, all_test_dataset, train_labels, all_test_labels = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "valid_dataset, test_dataset, valid_labels, test_labels =  train_test_split(all_test_dataset, all_test_labels, test_size=0.5)\n",
    "\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = reformat_lables(train_labels)\n",
    "valid_labels = reformat_lables(valid_labels)\n",
    "test_labels = reformat_lables(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (10595, 256) (10595, 2)\n",
      "Validation set (1324, 256) (1324, 2)\n",
      "Test set (1325, 256) (1325, 2)\n"
     ]
    }
   ],
   "source": [
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial logistic regression using stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "features_size = train_dataset.shape[1]\n",
    "num_labels = train_labels.shape[1]\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, features_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "    # Variables.\n",
    "    weights = tf.Variable(tf.truncated_normal([features_size, num_labels]))\n",
    "    biases = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "    # Training computation.\n",
    "    logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "    \n",
    "\n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 11.647083\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 54.2%\n",
      "Minibatch loss at step 500: 0.043880\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 93.1%\n",
      "Minibatch loss at step 1000: 0.176103\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 93.5%\n",
      "Minibatch loss at step 1500: 0.096478\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 93.4%\n",
      "Minibatch loss at step 2000: 0.269087\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 93.5%\n",
      "Minibatch loss at step 2500: 0.409628\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 90.7%\n",
      "Minibatch loss at step 3000: 0.340358\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 92.6%\n",
      "Test accuracy: 92.7%\n",
      "(1325, 2)\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "predictions = None\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "            # Pick an offset within the training data, which has been randomized.\n",
    "            # Note: we could use better randomization across epochs.\n",
    "            offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "            # Generate a minibatch.\n",
    "            batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "            batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "            # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "            # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "            # and the value is the numpy array to feed to it.\n",
    "            feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "            _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "            if (step % 500 == 0):\n",
    "                print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "                print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "                print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))\n",
    "    print(test_prediction.eval().shape)\n",
    "    predictions = test_prediction.eval()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9267924528301886\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[650,  72],\n",
       "       [ 25, 578]], dtype=int64)"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = np.argmax(predictions, 1) \n",
    "label = np.dot(test_labels, range(num_labels))\n",
    "print(accuracy_score(pred, label))\n",
    "confusion_matrix(pred, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized confusion matrix\n",
      "[[0.96296296 0.03703704]\n",
      " [0.11076923 0.88923077]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_confusion_matrix(confusion_matrix(label, pred), ['a','b'], normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-hidden layer neural network with rectified linear units nn.relu() and 1024 hidden nodes. using stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "features_size = train_dataset.shape[1]\n",
    "num_labels = train_labels.shape[1]\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, features_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    \n",
    "    # Variables.\n",
    "    hidden_layer_size = 1024\n",
    "    hidden_weights = tf.Variable(tf.truncated_normal([features_size, hidden_layer_size]))\n",
    "    hidden_biases = tf.Variable(tf.zeros([hidden_layer_size]))\n",
    "    \n",
    "    hidden_logits = tf.nn.relu(tf.matmul(tf_train_dataset, hidden_weights) + hidden_biases)\n",
    "\n",
    "    # Variables.\n",
    "    weights = tf.Variable(tf.truncated_normal([hidden_layer_size, num_labels]))\n",
    "    biases = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "    # Training computation.\n",
    "    logits = tf.matmul(hidden_logits, weights) + biases\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "\n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    hidden_valid_weights = tf.nn.relu(tf.matmul(tf_valid_dataset, hidden_weights) + hidden_biases)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(hidden_valid_weights, weights) + biases)\n",
    "    hidden_test_weights = tf.nn.relu(tf.matmul(tf_test_dataset, hidden_weights) + hidden_biases)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(hidden_test_weights, weights) + biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 423.857941\n",
      "Minibatch accuracy: 37.5%\n",
      "Validation accuracy: 49.8%\n",
      "Minibatch loss at step 500: 0.042121\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 89.6%\n",
      "Minibatch loss at step 1000: 0.017790\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.1%\n",
      "Minibatch loss at step 1500: 0.069031\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 89.3%\n",
      "Minibatch loss at step 2000: 0.010585\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 90.9%\n",
      "Minibatch loss at step 2500: 0.099613\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 90.9%\n",
      "Minibatch loss at step 3000: 0.000005\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 91.1%\n",
      "Test accuracy: 91.8%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "            # Pick an offset within the training data, which has been randomized.\n",
    "            # Note: we could use better randomization across epochs.\n",
    "            offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "            # Generate a minibatch.\n",
    "            batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "            batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "            # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "            # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "            # and the value is the numpy array to feed to it.\n",
    "            feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "            _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "            if (step % 500 == 0):\n",
    "                print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "                print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "                print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
