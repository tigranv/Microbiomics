{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import pandas as pd \n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from itertools import product\n",
    "import random\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "# data_root = \"C:\\\\Users\\\\Tigran PC\\\\Desktop\\\\MicrobiomicsData\\\\data\\\\\" # notebook\n",
    "# data_root = \"/Users/tigran/Desktop/sbv/data/\" # imac\n",
    "data_root = \"D:\\\\sbv Microbiomics\\\\data\\\\\" # istc pc\n",
    "\n",
    "sub_data_root = data_root + \"sample01split/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reformat_lables(labels):\n",
    "    num_labels = len(np.unique(labels))\n",
    "    # Map 0 to [1.0, 0.0, 0.0 ...], 1 to [0.0, 1.0, 0.0 ...]\n",
    "    labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "    return labels\n",
    "\n",
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1)) / predictions.shape[0])\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First reload the data we generated in Feature_Engineering.ipynb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (85390, 257)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = data_root + \"DNA_data.pickle\"\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    save = pickle.load(f)\n",
    "    all_dataset = save.astype(np.float32)\n",
    "    del save  # hint to help gc free up memory\n",
    "    print('Training set', all_dataset.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
      "  warnings.warn(\"Numerical issues were encountered \"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. \n",
      "  warnings.warn(\"Numerical issues were encountered \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (68312, 256) (68312,)\n",
      "Validation set (8539, 256) (8539,)\n",
      "Test set (8539, 256) (8539,)\n"
     ]
    }
   ],
   "source": [
    "X = all_dataset.values[:, :-1]\n",
    "X = preprocessing.scale(X)\n",
    "y = all_dataset.values[:, -1]\n",
    "\n",
    "train_dataset, all_test_dataset, train_labels, all_test_labels = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "valid_dataset, test_dataset, valid_labels, test_labels =  train_test_split(all_test_dataset, all_test_labels, test_size=0.5)\n",
    "\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = reformat_lables(train_labels)\n",
    "valid_labels = reformat_lables(valid_labels)\n",
    "test_labels = reformat_lables(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (68312, 256) (68312, 2)\n",
      "Validation set (8539, 256) (8539, 2)\n",
      "Test set (8539, 256) (8539, 2)\n"
     ]
    }
   ],
   "source": [
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial logistic regression using stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "features_size = train_dataset.shape[1]\n",
    "num_labels = train_labels.shape[1]\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, features_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "    # Variables.\n",
    "    weights = tf.Variable(tf.truncated_normal([features_size, num_labels]))\n",
    "    biases = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "    # Training computation.\n",
    "    logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "    \n",
    "\n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 8.782358\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy: 54.9%\n",
      "Minibatch loss at step 500: 0.675610\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 88.5%\n",
      "Minibatch loss at step 1000: 0.293927\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 88.9%\n",
      "Minibatch loss at step 1500: 0.265270\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 87.6%\n",
      "Minibatch loss at step 2000: 0.783421\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 88.6%\n",
      "Minibatch loss at step 2500: 0.206372\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 88.4%\n",
      "Minibatch loss at step 3000: 0.084122\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 88.5%\n",
      "Test accuracy: 89.7%\n",
      "(8539, 2)\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "predictions = None\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "            # Pick an offset within the training data, which has been randomized.\n",
    "            # Note: we could use better randomization across epochs.\n",
    "            offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "            # Generate a minibatch.\n",
    "            batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "            batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "            # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "            # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "            # and the value is the numpy array to feed to it.\n",
    "            feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "            _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "            if (step % 500 == 0):\n",
    "                print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "                print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "                print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))\n",
    "    print(test_prediction.eval().shape)\n",
    "    predictions = test_prediction.eval()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8968263262677129\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[3913,  405],\n",
       "       [ 476, 3745]], dtype=int64)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = np.argmax(predictions, 1) \n",
    "label = np.dot(test_labels, range(num_labels))\n",
    "print(accuracy_score(pred, label))\n",
    "confusion_matrix(pred, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized confusion matrix\n",
      "[[0.89154705 0.10845295]\n",
      " [0.09759036 0.90240964]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_confusion_matrix(confusion_matrix(label, pred), ['a','b'], normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-hidden layer neural network with rectified linear units nn.relu() and 1024 hidden nodes. using stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "features_size = train_dataset.shape[1]\n",
    "num_labels = train_labels.shape[1]\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, features_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    \n",
    "    # Variables.\n",
    "    hidden_layer_size = 1024\n",
    "    hidden_weights = tf.Variable(tf.truncated_normal([features_size, hidden_layer_size]))\n",
    "    hidden_biases = tf.Variable(tf.zeros([hidden_layer_size]))\n",
    "    \n",
    "    hidden_logits = tf.nn.relu(tf.matmul(tf_train_dataset, hidden_weights) + hidden_biases)\n",
    "\n",
    "    # Variables.\n",
    "    weights = tf.Variable(tf.truncated_normal([hidden_layer_size, num_labels]))\n",
    "    biases = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "    # Training computation.\n",
    "    logits = tf.matmul(hidden_logits, weights) + biases\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "\n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    hidden_valid_weights = tf.nn.relu(tf.matmul(tf_valid_dataset, hidden_weights) + hidden_biases)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(hidden_valid_weights, weights) + biases)\n",
    "    hidden_test_weights = tf.nn.relu(tf.matmul(tf_test_dataset, hidden_weights) + hidden_biases)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(hidden_test_weights, weights) + biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 126.284805\n",
      "Minibatch accuracy: 50.0%\n",
      "Validation accuracy: 47.9%\n",
      "Minibatch loss at step 500: 0.595899\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 84.9%\n",
      "Minibatch loss at step 1000: 0.226084\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 86.6%\n",
      "Minibatch loss at step 1500: 0.425552\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 86.2%\n",
      "Minibatch loss at step 2000: 2.016596\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 83.2%\n",
      "Minibatch loss at step 2500: 0.337159\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 87.5%\n",
      "Minibatch loss at step 3000: 0.531490\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 87.5%\n",
      "Test accuracy: 86.8%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "            # Pick an offset within the training data, which has been randomized.\n",
    "            # Note: we could use better randomization across epochs.\n",
    "            offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "            # Generate a minibatch.\n",
    "            batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "            batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "            # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "            # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "            # and the value is the numpy array to feed to it.\n",
    "            feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "            _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "            if (step % 500 == 0):\n",
    "                print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "                print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "                print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
